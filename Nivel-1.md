# üöÄ Bienvenido al Nivel 1 del Roadmap Lakehouse

**El primer paso real hacia dominar la ingenier√≠a de datos moderna.**

Dejamos atr√°s los ejemplos triviales: a partir de ahora **pensar√°s y trabajar√°s como un Data Engineer real**.  
Aprender√°s a usar **Python como el motor central del proceso ETL**, procesando datos masivos con prop√≥sito, eficiencia y mentalidad de producci√≥n.

Cada m√≥dulo, funci√≥n y script tendr√° un objetivo claro dentro del flujo **Extract ‚Üí Transform ‚Üí Load**, y se integrar√° dentro del modelo **Lakehouse (Bronze ‚Üí Silver ‚Üí Gold)** ‚Äî la arquitectura base de las plataformas modernas de datos.

---

## üß© Nivel 1 ‚Äî ETL B√°sico con Python + MinIO

**‚ÄúDe scripts aislados a un pipeline estructurado en el Lakehouse.‚Äù**

---

### üí° Introducci√≥n

Aqu√≠ comienza tu entrenamiento pr√°ctico üí™:  
aprender√°s a **dise√±ar y construir un pipeline ETL real con Python**, conectado a **MinIO como Data Lake local**, y estructurado bajo el modelo **Bronze / Silver / Gold**.

Esta es la misma arquitectura utilizada por compa√±√≠as como Databricks, Snowflake o Uber ‚Äî pero adaptada a un entorno controlado, local y 100 % reproducible.

---

### üéØ Objetivo del Nivel 1

> Construir un **pipeline ETL completo con Python**, utilizando datos reales (NYC Taxi), almacenados en **MinIO** a trav√©s de las capas **Bronze, Silver y Gold**, con control de memoria, manejo eficiente de datos y almacenamiento columnar (Parquet).

Este nivel desarrolla tu **mentalidad de ingenier√≠a de datos**: crear pipelines **modulares, trazables y configurables**, antes de escalar hacia sistemas distribuidos.

---

### ‚öôÔ∏è Enfoque general

Aprender√°s a:

1. **Dise√±ar un pipeline ETL por capas (Bronze ‚Üí Silver ‚Üí Gold)**.
    
2. **Procesar datasets grandes (‚â• 1 GB)** enfrentando l√≠mites reales de hardware.
    
3. **Optimizar rendimiento y memoria** con herramientas nativas de Python (`csv`, `gzip`, `pyarrow`, `multiprocessing`, etc.).
    
4. **Persistir resultados** en **MinIO (Parquet)** y **PostgreSQL**.
    
5. **Monitorear, versionar y medir** tu pipeline como en un entorno productivo.
    

---

## üî® Estructura del Lab ‚Äî ‚ÄúETL Lakehouse con Python‚Äù

### üå∞ Capa Bronze ‚Äî Raw Zone (Extract)

- Fuente: archivos `yellow_taxi_data_*.csv` ubicados en tu **bucket `lakehouse/bronze/`** en MinIO o desde un origen p√∫blico.
    
- Extracci√≥n con librer√≠as est√°ndar (`csv`, `requests`, `gzip`) o herramientas ETL en Python.
    
- **Medici√≥n de rendimiento y memoria** con `perf_counter` (tiempos) y `psutil` o `memory_profiler` (RAM).

üëâ **Objetivo:** extraer y almacenar los datos crudos sin alterarlos en la capa _Bronze_ del Lakehouse.

---

### ‚öôÔ∏è Capa Silver ‚Äî Clean Zone (Transform + Validate)

Aplicar√°s transformaciones y validaciones para limpiar y estandarizar los datos antes de modelarlos.  
Aqu√≠ puedes usar **Pandas**, **Polars** o funciones nativas de Python seg√∫n el tama√±o del dataset.

#### üß© Transformaciones b√°sicas

- Conversi√≥n de timestamps ‚Üí `datetime`.
    
- C√°lculo de duraci√≥n del viaje (minutos).
    
- Filtrado de outliers (viajes negativos, distancias > 100 km).
    
- Creaci√≥n de columnas derivadas (`fare_per_km`, `is_airport_trip`).
    
- Agrupaciones y agregaciones simples.
    
#### üß™ Validaci√≥n de calidad

- Comprobaci√≥n de esquema y tipos.
    
- Detecci√≥n de nulos, duplicados o valores fuera de rango.
    
- Validaci√≥n de reglas de negocio (`fare_amount > 0`).
    
- Detecci√≥n de anomal√≠as (`describe()`, IQR, z-score).

#### ‚ö° Medici√≥n de rendimiento y memoria

As√≠ como en la extracci√≥n, esta capa tambi√©n debe medirse:

- Usa `perf_counter` para identificar cuellos de botella en las transformaciones.
    
- Emplea `psutil` o `memory_profiler` para revisar el uso de RAM durante operaciones intensivas como `merge` o `groupby`.


üëâ **Objetivo:** generar una versi√≥n **depurada, estructurada y lista para an√°lisis**.

---

### üèÜ Capa Gold ‚Äî Curated Zone (Load)

Con los datos ya limpios, crear√°s tablas anal√≠ticas y datasets curados.

**Ejercicios:**

- Generar agregados por zona, hora o tipo de pago.
    
- Exportar resultados a **`lakehouse/gold/`** (Parquet).
    
- Cargar la tabla final `fact_trips` en **PostgreSQL**.
    
- Comparar tama√±os y tiempos frente a CSV.
    

üëâ **Objetivo:** construir la **capa de datos modelada y lista para consumo anal√≠tico.**

---

### üß∞ Configuraci√≥n del pipeline (Config-driven)

Gestiona par√°metros y rutas desde un `config.yml`:

```yaml
lakehouse:
  endpoint: "http://localhost:9000"
  access_key: "minio"
  secret_key: "minio123"
  buckets:
    bronze: "lakehouse/bronze/"
    silver: "lakehouse/silver/"
    gold: "lakehouse/gold/"
etl:
  chunksize: 500000
  output_format: "parquet"
  validation: true
```

üëâ **Objetivo:** crear un pipeline **parametrizable y reutilizable**, sin hardcoding.

---

### üß© Modularizaci√≥n del c√≥digo

Estructura tu proyecto siguiendo una arquitectura limpia:

```
etl_pipeline/
‚îú‚îÄ‚îÄ extract.py
‚îú‚îÄ‚îÄ transform.py
‚îú‚îÄ‚îÄ validate.py
‚îú‚îÄ‚îÄ load.py
‚îú‚îÄ‚îÄ lakehouse.py      # gesti√≥n de MinIO y conexi√≥n S3
‚îú‚îÄ‚îÄ utils.py
‚îî‚îÄ‚îÄ main.py           # orquestador del flujo ETL
```

üëâ **Objetivo:** crear c√≥digo **limpio, desacoplado y mantenible**, propio de un entorno profesional.

---

### üíæ Versionamiento y control de datos

Cada ejecuci√≥n del pipeline:

- Genera una versi√≥n en cada capa (`bronze`, `silver`, `gold`) con timestamp.
    
- Registra metadatos (fecha, filas, tama√±o, hash) en `lakehouse/_metadata.csv`.
    

üëâ **Objetivo:** aplicar **data lineage y trazabilidad** entre las capas del Lakehouse.

---

### ‚ö° Benchmarking comparativo

Eval√∫a distintos enfoques t√©cnicos:

|Escenario|Tiempo (s)|Memoria (MB)|Tama√±o (MB)|
|---|---|---|---|
|Bronze (CSV)|220|1400|900|
|Silver (Parquet)|95|680|180|
|Gold (Agregado)|45|500|60|

üëâ **Objetivo:** medir el impacto de tus decisiones t√©cnicas a lo largo del pipeline.

---

### üîç Observabilidad b√°sica

- Logging con `logging` o `structlog`.
    
- Barra de progreso con `tqdm`.
    
- Monitoreo de consumo de CPU/RAM (`psutil`).
    
- Exportaci√≥n de m√©tricas de ejecuci√≥n (tiempo, tama√±o, versi√≥n).
    

üëâ **Objetivo:** aprender a **observar y diagnosticar** tu pipeline como un sistema vivo.

---

### ‚öôÔ∏è Bonus avanzado

- **Parallel processing** con `ThreadPoolExecutor` o `multiprocessing`.
    
- **Checkpointing** por capa (reanudar desde Silver).
    
- **Data sampling** para pruebas r√°pidas.
    

üëâ **Objetivo:** incorporar **resiliencia y eficiencia** desde el inicio.

---

## üß† Conceptos introducidos

|Concepto|Descripci√≥n|
|---|---|
|**Lakehouse architecture**|Capas Bronze / Silver / Gold|
|**ETL modular en Python**|Dise√±o por funciones y m√≥dulos|
|**Chunked processing**|Procesamiento por bloques controlados|
|**Data validation**|Control de integridad y negocio|
|**Config-driven pipeline**|Separar configuraci√≥n del c√≥digo|
|**Data lineage**|Versionado y trazabilidad entre capas|
|**Benchmarking**|Evaluaci√≥n de rendimiento|
|**MinIO integration**|Simulaci√≥n de un Data Lake S3-compatible|

---

## üß™ Ejercicio Final ‚Äî `etl_nyc_lakehouse.py`

> Construye tu **pipeline ETL Lakehouse completo con Python y MinIO.**

Tu script deber√°:

1. Extraer datos crudos ‚Üí **Bronze**.
    
2. Transformar y validar ‚Üí **Silver**.
    
3. Agregar y publicar ‚Üí **Gold**.
    
4. Cargar la capa Gold a **PostgreSQL**.
    
5. Registrar m√©tricas y versiones.
    

üéØ **Resultado esperado:** un pipeline **estructurado, trazable y reproducible**, aplicando buenas pr√°cticas de ingenier√≠a de datos con Python.

---

## üìä Evaluaci√≥n del Nivel

|Criterio|Peso|Logrado|
|---|---|---|
|Pipeline por capas (Bronze/Silver/Gold)|25 %|‚òê|
|Transformaciones y validaciones reales|20 %|‚òê|
|Carga en MinIO y PostgreSQL|15 %|‚òê|
|Optimizaci√≥n y benchmarking|15 %|‚òê|
|Observabilidad y m√©tricas|10 %|‚òê|
|Modularizaci√≥n y configuraci√≥n externa|10 %|‚òê|
|Versionamiento y lineage|5 %|‚òê|

---

## üöÄ Transici√≥n al Nivel 2 ‚Äî ETL Optimizado con Polars

> Al finalizar este nivel, habr√°s construido un **pipeline ETL real con Python**, basado en el modelo Lakehouse y respaldado por MinIO.  
> En el **Nivel 2**, escalar√°s este dise√±o con **Polars**, **paralelismo nativo** y **rendimiento extremo**.

---


# üß± Comenzando el Nivel 1 ‚Äî Construyendo un ETL real con Python

En el **Nivel 0** ya descubriste las bases del procesamiento de datos en Python:  
aprendiste a trabajar con grandes vol√∫menes de informaci√≥n, a manipularlos con eficiencia (`FULL LOAD` vs `chunksize`) y a manejar la memoria como lo har√≠a un ingeniero de datos.

Ahora vamos un paso m√°s all√°.  
En este **Nivel 1**, transformar√°s ese conocimiento en un **pipeline ETL completo y estructurado**, aplicando principios reales de ingenier√≠a de datos ‚Äî y usando **Pandas como motor de ejecuci√≥n**, no como el objetivo principal.

---

## üéØ De ‚Äúprocesar datos‚Äù a ‚Äúconstruir pipelines‚Äù

Hasta ahora, el foco estuvo en cargar y explorar datos dentro de Python.  
A partir de este punto, aprender√°s a **dise√±ar, modularizar y ejecutar un flujo ETL completo**:

> **Extract ‚Üí Transform ‚Üí Load**

Cada etapa se construye con prop√≥sito, control de recursos, validaciones y configuraciones externas, tal como se har√≠a en un entorno productivo.

---

## üîç ¬øQu√© cambia con respecto al Nivel 0?

|Nivel 0 (Fundamentos de Python)|Nivel 1 (Ingenier√≠a de datos con Python)|
|---|---|
|Ejecuci√≥n manual en notebooks|Pipeline modular y automatizado|
|Procesamiento secuencial simple|Extracci√≥n y carga optimizadas|
|Sin reglas de negocio|Transformaciones tipadas y validadas|
|Sin control de calidad|M√©tricas y validaciones autom√°ticas|
|Scripts r√≠gidos|C√≥digo configurable y parametrizable|
|Resultados temporales|Persistencia estructurada (Parquet / SQL)|

---

## üß© Estructura del Lab ‚Äî ‚ÄúETL con Python‚Äù

El laboratorio de este nivel te guiar√° por las tres etapas principales de un flujo profesional:

### 1Ô∏è‚É£ Extracci√≥n (Extract)

- Fuente: archivos `yellow_taxi_data_*.csv` desde un bucket p√∫blico a tu MinIO local.
    
- Lectura por **bloques (`chunksize`)** para simular _streaming batch_.
    
- **Medici√≥n de rendimiento y memoria** con `perf_counter` (tiempos) y `psutil` o `memory_profiler` (RAM).
    
- Implementaci√≥n de logs b√°sicos (`logging`) para registrar progreso y eventos.
    

üëâ **Objetivo:** comprender que **la extracci√≥n es una fase cr√≠tica del ETL**, donde el control de recursos, la limpieza inicial y el registro de m√©tricas determinan la estabilidad del pipeline.

---

### üß© Modularizaci√≥n aplicada a la Extracci√≥n

A diferencia del Nivel 0 ‚Äîdonde escrib√≠as c√≥digo en un solo notebook‚Äî aqu√≠ comenzar√°s a pensar **como un ingeniero de datos en Python**, dividiendo responsabilidades por m√≥dulo.

Tu estructura base ser√°:

```
etl_pipeline/
‚îú‚îÄ‚îÄ extract.py       # L√≥gica de lectura y extracci√≥n (chunksize, logs, m√©tricas)
‚îú‚îÄ‚îÄ transform.py     # Transformaciones del dataset
‚îú‚îÄ‚îÄ validate.py      # Validaciones y control de calidad
‚îú‚îÄ‚îÄ load.py          # Carga a PostgreSQL o Parquet
‚îú‚îÄ‚îÄ utils.py         # Funciones comunes (rutas, tiempos, logs)
‚îî‚îÄ‚îÄ main.py          # Orquestador general del pipeline
```

> En esta primera etapa trabajaremos principalmente en **extract.py** y **main.py**, dejando el resto preparado para extenderlo m√°s adelante.

üëâ **Objetivo:** construir c√≥digo **modular, reutilizable y legible**, donde cada parte tenga una funci√≥n clara dentro del pipeline.

---

## ‚öôÔ∏è Dise√±o con configuraci√≥n externa

Evita valores fijos en tu c√≥digo.  
A partir de este nivel, aprender√°s a **desacoplar la configuraci√≥n** del pipeline mediante un archivo `config.yml`:

```yaml
etl:
  chunksize: 500000
  input_path: "lakehouse/bronze/"
  output_path: "lakehouse/silver/"
  output_format: "parquet"

lakehouse:
  endpoint: "http://localhost:9000"
  access_key: "minio"
  secret_key: "minio123"
```

üëâ **Objetivo:** convertir tu pipeline Python en un sistema **parametrizable y portable**, f√°cilmente reutilizable por otros entornos o datasets.

---

## üß† Antes de continuar

Antes de avanzar hacia la **Transformaci√≥n**, aseg√∫rate de:

- Haber probado correctamente la lectura incremental (`chunksize`).
    
- Medir y registrar tiempos y consumo de memoria.
    
- Confirmar que los tipos de columnas y nombres se mantienen consistentes.
    
- Contar con un sistema de logging que capture cada ejecuci√≥n.
    
- Tener tu estructura modular operativa: `main.py` debe orquestar la extracci√≥n.
    

---

> üí¨ **Reflexi√≥n:**  
> Python es hoy el lenguaje central de la ingenier√≠a de datos moderna.  
> Librer√≠as como Pandas, Polars, PyArrow o Dask son solo motores:  
> lo importante es **entender los principios de dise√±o, modularizaci√≥n, validaci√≥n y trazabilidad**.
> 
> Con este nivel, est√°s aprendiendo a pensar como un ingeniero de datos que domina **Python como plataforma de orquestaci√≥n**, no como una simple herramienta de an√°lisis.

---
